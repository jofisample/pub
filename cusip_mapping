
# -----------------------------
# Model Initialization (in Main Execution)
# -----------------------------
mpnet_path = os.path.join(modelpath, "all-mpnet-base-v2")
mpnet = SentenceTransformer(mpnet_path)

finbert_path = os.path.join(modelpath, "ProsusAI-finbert")
finbert = pipeline("sentiment-analysis", model=finbert_path)

# -----------------------------
# Module 0: Download NLTK Resources
# -----------------------------
nltk.download('punkt', quiet=True)

# -----------------------------
# Module 1: Filing Text Retrieval
# -----------------------------
def pull_filing(cik, accno):
    """
    Given a company identifier (cik) and accession number (accno),
    load the filing pickle and return the text from the relevant SEC item.
    """
    dic = {"10-K": "item1a", "10-Q": "2item1a"}
    # Here we assume filings are stored in a folder; adjust path as needed.
    filing_path = os.path.join(modelpath, f"{cik}_{accno}.pkl")
    filing = pd.read_pickle(filing_path)
    text = filing["items"].get(dic.get(filing["formtype"]))
    return text

# -----------------------------
# Module 2: Sentence Tokenization
# -----------------------------
def split_into_sentences(text):
    """
    Tokenize the provided text into a list of sentences.
    """
    if text:
        return nltk.sent_tokenize(text)
    return []

# -----------------------------
# Module 3: Sentence Categorization with difflib (Phase 1)
# -----------------------------
def categorize_sentence_changes(prev_sentences, curr_sentences, base_threshold=0.7, unchanged_threshold=0.95, model):
    """
    Categorize sentence changes between previous and current filings.
    
    First, use difflib.SequenceMatcher to quickly detect blocks of exactly identical sentences.
    Exact matches are added to "unchanged" and removed from further processing.
    
    Then, for remaining sentences, compute embeddings using the provided model and assign:
      - "added": current sentences with max similarity < base_threshold.
      - "altered": current sentences with max similarity in [base_threshold, unchanged_threshold).
      - "unchanged": both exact matches and current sentences with similarity >= unchanged_threshold.
      - "removed": previous sentences (excluding exact matches) with no similar match.
    """
    # Find exact matches with difflib
    matcher = difflib.SequenceMatcher(None, prev_sentences, curr_sentences)
    matching_blocks = matcher.get_matching_blocks()
    exact_unchanged = set()
    for match in matching_blocks:
        if match.size > 0:
            for i in range(match.b, match.b + match.size):
                exact_unchanged.add(curr_sentences[i])
    exact_unchanged = list(exact_unchanged)
    
    filtered_curr = [s for s in curr_sentences if s not in exact_unchanged]
    filtered_prev = [s for s in prev_sentences if s not in exact_unchanged]
    
    categories = {"added": [], "unchanged": exact_unchanged.copy(), "altered": [], "removed": []}
    
    if filtered_prev and filtered_curr:
        emb_prev = model.encode(filtered_prev)
        emb_curr = model.encode(filtered_curr)
        similarity_matrix = util.cos_sim(emb_curr, emb_prev).numpy()
        
        for i, sims in enumerate(similarity_matrix):
            max_sim = np.max(sims)
            if max_sim < base_threshold:
                categories["added"].append(filtered_curr[i])
            else:
                if max_sim >= unchanged_threshold:
                    categories["unchanged"].append(filtered_curr[i])
                else:
                    categories["altered"].append(filtered_curr[i])
                    
        col_maxes = np.max(similarity_matrix, axis=0)
        for j, sim in enumerate(col_maxes):
            if sim < base_threshold:
                categories["removed"].append(filtered_prev[j])
    else:
        for s in prev_sentences:
            if s not in exact_unchanged:
                categories["removed"].append(s)
                
    return categories

def process_single_row(row, model):
    """
    Process a single DataFrame row to generate categorized sentences.
    Returns a tuple (cik, pair_key, category_dict) if valid; else None.
    """
    cik = row["cik"]
    accno = row["accno"]
    prev_filing_accno = row["prev_filing_accno"]
    pair_key = f"{prev_filing_accno}_{accno}"
    
    text_prev = pull_filing(cik, prev_filing_accno)
    text_curr = pull_filing(cik, accno)
    prev_sentences = split_into_sentences(text_prev)
    curr_sentences = split_into_sentences(text_curr)
    
    if not prev_sentences or not curr_sentences:
        return None
    category_dict = categorize_sentence_changes(prev_sentences, curr_sentences,
                                               base_threshold=0.7,
                                               unchanged_threshold=0.95,
                                               model=model)
    return (cik, pair_key, category_dict)

def process_filings_categories_parallel(df, model):
    """
    Process the DataFrame rows in parallel to generate a nested dictionary of categorized sentences.
    Returns a dictionary keyed by cik then filing pair.
    """
    rows = df.to_dict('records')
    results_list = Parallel(n_jobs=-1)(
        delayed(process_single_row)(row, model) for row in rows
    )
    results = {}
    for item in results_list:
        if item is None:
            continue
        cik, pair_key, category_dict = item
        if cik not in results:
            results[cik] = {}
        results[cik][pair_key] = category_dict
    return results

# -----------------------------
# Module 4b: FinBERT Sentiment Analysis (Phase 2)
# -----------------------------
def analyze_sentiment(sentences, finbert_pipeline):
    """
    Analyze sentiment of a list of sentences using FinBERT.
    Returns a dictionary mapping each sentence to its FinBERT sentiment output.
    """
    if not sentences:
        return {}
    sentiments = finbert_pipeline(sentences)
    return {sentence: sentiment for sentence, sentiment in zip(sentences, sentiments)}

def process_filing_pair_sentiments(pair_tuple, finbert_pipeline):
    """
    Process one filing pair given as a tuple (cik, pair_key, categories).
    Returns a tuple (cik, pair_key, sentiment_results) where sentiment_results is a dict.
    Skips the "unchanged" category.
    """
    cik, pair_key, categories = pair_tuple
    sentiment_results = {}
    for category, sentences in categories.items():
        if category == "unchanged":
            sentiment_results[category] = {}
        else:
            sentiment_results[category] = analyze_sentiment(sentences, finbert_pipeline)
    return (cik, pair_key, sentiment_results)

def process_sentiments_without_gpt_parallel(categorized_dict, finbert_pipeline):
    """
    Process the categorized dictionary in parallel to compute FinBERT sentiment results.
    Returns a nested dictionary where for each filing pair (keyed by cik and pair_key)
    each category (except "unchanged") maps each sentence to its FinBERT sentiment.
    """
    # Flatten the dictionary into a list of tuples for parallel processing.
    pairs = []
    for cik, filing_dict in categorized_dict.items():
        for pair_key, categories in filing_dict.items():
            pairs.append((cik, pair_key, categories))
    
    results_list = Parallel(n_jobs=-1)(
        delayed(process_filing_pair_sentiments)(pair, finbert_pipeline) for pair in pairs
    )
    results = {}
    for cik, pair_key, sentiment_results in results_list:
        if cik not in results:
            results[cik] = {}
        results[cik][pair_key] = sentiment_results
    return results

# -----------------------------
# Module 5: GPT-4 Turbo Correction (Phase 3)
# -----------------------------
def correct_sentiments_with_gpt(sentiment_dict):
    """
    For each sentence in sentiment_dict (mapping sentence to FinBERT sentiment),
    call GPT-4 Turbo to determine the correct sentiment.
    Expects GPT-4 Turbo to return exactly one word: positive, negative, or neutral.
    Returns a new dictionary mapping each sentence to a corrected sentiment record.
    """
    corrected = {}
    for sentence, sentiment in sentiment_dict.items():
        current_label = sentiment["label"]
        prompt = (
            f"Given the following sentence from a financial document:\n\n"
            f"\"{sentence}\"\n\n"
            "In a financial context, please determine the sentiment. "
            "Respond with exactly one word—either positive, negative, or neutral—and nothing else."
        )
        gpt_response = invoke_completions(prompt)
        corrected_label = gpt_response.strip().lower()
        if corrected_label not in ["positive", "negative", "neutral"]:
            corrected_label = current_label
        corrected[sentence] = {
            "original_label": current_label,
            "corrected_label": corrected_label,
            "gpt_response": gpt_response.strip()
        }
    return corrected

def process_filing_pair_correction(pair_tuple):
    """
    Process one filing pair's sentiment dictionary (from Phase 2) by applying GPT correction.
    pair_tuple: (cik, pair_key, category_sentiments)
    Returns: (cik, pair_key, corrected_results)
    """
    cik, pair_key, category_sentiments = pair_tuple
    corrected_category = {}
    for category, sentiment_dict in category_sentiments.items():
        if sentiment_dict:
            corrected_category[category] = correct_sentiments_with_gpt(sentiment_dict)
        else:
            corrected_category[category] = {}
    return (cik, pair_key, corrected_category)

def process_sentiments_correction_parallel(sentiments_dict):
    """
    Process the FinBERT sentiment results (from Phase 2) in parallel by applying GPT correction.
    Returns an updated nested dictionary with GPT-corrected sentiment results.
    """
    pairs = []
    for cik, filing_dict in sentiments_dict.items():
        for pair_key, category_sentiments in filing_dict.items():
            pairs.append((cik, pair_key, category_sentiments))
            
    results_list = Parallel(n_jobs=-1)(
        delayed(process_filing_pair_correction)(pair) for pair in pairs
    )
    corrected_results = {}
    for cik, pair_key, corrected_category in results_list:
        if cik not in corrected_results:
            corrected_results[cik] = {}
        corrected_results[cik][pair_key] = corrected_category
    return corrected_results

# -----------------------------
# Module 6b: Aggregation (Phase 4)
# -----------------------------
def aggregate_sentiment_scores(sentiment_dict):
    """
    Given a dictionary of sentiment results per category,
    aggregate a numeric score.
    Mapping: positive -> +1, neutral -> 0, negative -> -1.
    Returns a dictionary with the aggregated average sentiment per category and list of scores.
    """
    sentiment_mapping = {"positive": 1, "neutral": 0, "negative": -1}
    aggregated = {}
    for category, sentence_sentiments in sentiment_dict.items():
        scores = []
        for sent, res in sentence_sentiments.items():
            label = res.get("corrected_label", "neutral")
            score = sentiment_mapping.get(label, 0)
            scores.append(score)
        avg = np.mean(scores) if scores else None
        aggregated[category] = {"avg_sentiment": avg, "scores": scores}
    return aggregated

def aggregate_for_all_filings(sentiment_dict):
    """
    Given the updated sentiment dictionary for all filings (keyed by cik and filing pair),
    aggregate sentiment scores for each filing pair.
    Returns a nested dictionary with aggregated sentiment scores.
    """
    aggregated_results = {}
    for cik, filing_dict in sentiment_dict.items():
        aggregated_results[cik] = {}
        for pair_key, cat_scores in filing_dict.items():
            aggregated_results[cik][pair_key] = aggregate_sentiment_scores(cat_scores)
    return aggregated_results

# -----------------------------
# Module 7: Weighting Strategy (Phase 5)
# -----------------------------
def compute_weighted_overall_sentiment(aggregated_results, weights=None):
    """
    Apply a weighting strategy to combine aggregated sentiment scores from different categories.
    Default weighting example:
      - "added": weight = 1.0 (new risks have full negative impact)
      - "altered": weight = 0.5 (moderate change)
      - "removed": weight = -1.0 (removal of negative risks is good news)
      - "unchanged": weight = 0.0 (no impact)
      
    For each filing pair, compute the weighted overall sentiment.
    Returns a nested dictionary keyed by cik and filing pair with the overall weighted sentiment.
    """
    if weights is None:
        weights = {"added": 1.0, "altered": 0.5, "removed": -1.0, "unchanged": 0.0}
    
    weighted_results = {}
    for cik, filing_dict in aggregated_results.items():
        weighted_results[cik] = {}
        for pair_key, cat_scores in aggregated_results[cik].items():
            total_weighted_score = 0.0
            total_abs_weight = 0.0
            for category, score_data in cat_scores.items():
                avg = score_data.get("avg_sentiment")
                if avg is not None:
                    w = weights.get(category, 0)
                    total_weighted_score += avg * w
                    total_abs_weight += abs(w)
            overall_sentiment = total_weighted_score / total_abs_weight if total_abs_weight > 0 else None
            weighted_results[cik][pair_key] = overall_sentiment
    return weighted_results


    # Load the DataFrame (ensure pklpath is set correctly)
    df = pd.read_pickle(pklpath + "f.pkl").reset_index(drop=True)
    
    # ----- Phase 1: Sentence Categorization -----
    categories_dict = process_filings_categories_parallel(df, model=mpnet)
    with open("filings_sentence_categories.pkl", "wb") as f:
        pickle.dump(categories_dict, f)
    print("Phase 1 complete: Categorized sentences saved.")
    
    # ----- Phase 2: FinBERT Sentiment Analysis (Without GPT Correction) -----
    sentiments_dict = process_sentiments_without_gpt_parallel(categories_dict, finbert_pipeline=finbert)
    with open("filings_sentiment_finbert.pkl", "wb") as f:
        pickle.dump(sentiments_dict, f)
    print("Phase 2 complete: FinBERT sentiment analysis saved.")
    
    # ----- Phase 3: (Optional) GPT-4 Turbo Correction -----
    sentiments_corrected = process_sentiments_correction_parallel(sentiments_dict)
    with open("filings_sentiment_corrected.pkl", "wb") as f:
        pickle.dump(sentiments_corrected, f)
    print("Phase 3 complete: GPT-4 Turbo sentiment corrections saved.")
    
    # ----- Phase 4: Aggregation -----
    aggregated_results = aggregate_for_all_filings(sentiments_corrected)
    with open("filings_sentiment_aggregated.json", "w") as f:
        json.dump(aggregated_results, f, indent=2)
    print("Phase 4 complete: Aggregated sentiment results saved.")
    
    # ----- Phase 5: Weighting Strategy -----
    weighted_overall = compute_weighted_overall_sentiment(aggregated_results)
    with open("filings_weighted_overall.json", "w") as f:
        json.dump(weighted_overall, f, indent=2)
    print("Phase 5 complete: Weighted overall sentiment results saved.")
    
    # Optionally, print a sample of the weighted overall results
    print(json.dumps(weighted_overall, indent=2))
