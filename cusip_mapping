
mpnet_path = os.path.join(models, "all-mpnet-base-v2")
mpnet = SentenceTransformer(mpnet_path)

finbert_path = os.path.join(models, "ProsusAI-finbert")
finbert = pipeline("sentiment-analysis", model=finbert_path)

nltk.download('punkt', quiet=True)

def pull_filing(cik, accno):
    dic = {"10-K": "item1a", "10-Q": "2item1a"}
    filing_path = os.path.join(parsedfolder, f"{cik}_{accno}.pkl")
    filing = pd.read_pickle(filing_path)
    text = filing["items"].get(dic.get(filing["formtype"]))
    return text

def split_into_sentences(text):
    if text:
        return nltk.sent_tokenize(text)
    return []

def categorize_sentence_changes(prev_sentences, curr_sentences, model, base_threshold=0.7, unchanged_threshold=0.95):
    matcher = difflib.SequenceMatcher(None, prev_sentences, curr_sentences)
    matching_blocks = matcher.get_matching_blocks()
    exact_unchanged = set()
    for match in matching_blocks:
        if match.size > 0:
            for i in range(match.b, match.b + match.size):
                exact_unchanged.add(curr_sentences[i])
    exact_unchanged = list(exact_unchanged)
    filtered_curr = [s for s in curr_sentences if s not in exact_unchanged]
    filtered_prev = [s for s in prev_sentences if s not in exact_unchanged]
    categories = {"added": [], "unchanged": exact_unchanged.copy(), "altered": [], "removed": []}
    if filtered_prev and filtered_curr:
        emb_prev = model.encode(filtered_prev)
        emb_curr = model.encode(filtered_curr)
        similarity_matrix = util.cos_sim(emb_curr, emb_prev).numpy()
        for i, sims in enumerate(similarity_matrix):
            max_sim = np.max(sims)
            if max_sim < base_threshold:
                categories["added"].append(filtered_curr[i])
            else:
                if max_sim >= unchanged_threshold:
                    categories["unchanged"].append(filtered_curr[i])
                else:
                    categories["altered"].append(filtered_curr[i])
        col_maxes = np.max(similarity_matrix, axis=0)
        for j, sim in enumerate(col_maxes):
            if sim < base_threshold:
                categories["removed"].append(filtered_prev[j])
    else:
        for s in prev_sentences:
            if s not in exact_unchanged:
                categories["removed"].append(s)
    return categories

def process_filing_row(row, model):
    cik = row["cik"]
    accno = row["accno"]
    prev_filing_accno = row["prev_filing_accno"]
    pair_key = f"{prev_filing_accno}_{accno}"
    text_prev = pull_filing(cik, prev_filing_accno)
    text_curr = pull_filing(cik, accno)
    prev_sentences = split_into_sentences(text_prev)
    curr_sentences = split_into_sentences(text_curr)
    if not prev_sentences or not curr_sentences:
        return None
    category_dict = categorize_sentence_changes(prev_sentences, curr_sentences, model=model, base_threshold=0.7, unchanged_threshold=0.95)
    return (cik, pair_key, category_dict)

def process_filings_categories_chunked(df, model, chunk_size=200, output_dir=".", existing_pkl_file=None):
    if existing_pkl_file is not None and os.path.exists(existing_pkl_file):
        with open(existing_pkl_file, "rb") as f:
            results = pickle.load(f)
    else:
        results = {}
    rows = df.to_dict(orient='records')
    total = len(rows)
    for i in range(0, total, chunk_size):
        chunk = rows[i:i+chunk_size]
        processed = Parallel(n_jobs=-1, backend="threading")(
            delayed(process_filing_row)(row, model=model) for row in tqdm(chunk, total=len(chunk), desc=f"Processing rows {i} to {i+len(chunk)}")
        )
        for res in processed:
            if res is None:
                continue
            cik, pair_key, category_dict = res
            if cik not in results:
                results[cik] = {}
            results[cik][pair_key] = category_dict
        timestamp = pd.Timestamp.today().strftime("%Y%m%d_%H%M%S")
        partial_filename = os.path.join(output_dir, f"filings_sentence_categories_{i}_{timestamp}.pkl")
        with open(partial_filename, "wb") as f:
            pickle.dump(results, f)
        print(f"Chunk {i} processed and saved to {partial_filename}.")
    return results

def analyze_sentiment(sentences, finbert_pipeline):
    if not sentences:
        return {}
    sentiments = finbert_pipeline(sentences)
    return {sentence: sentiment for sentence, sentiment in zip(sentences, sentiments)}

def process_sentiments_without_gpt(categorized_dict, finbert_pipeline):
    results = {}
    for cik, filing_dict in tqdm(list(categorized_dict.items()), desc="Processing sentiments for filings"):
        results[cik] = {}
        for pair_key, categories in filing_dict.items():
            sentiment_results = {}
            for category, sentences in categories.items():
                if category == "unchanged":
                    sentiment_results[category] = {s: {"label": "unchanged", "score": None} for s in sentences}
                else:
                    sentiment_results[category] = analyze_sentiment(sentences, finbert_pipeline)
            results[cik][pair_key] = sentiment_results
    return results

def correct_sentiments_with_gpt(sentiment_dict):
    corrected = {}
    for sentence, sentiment in sentiment_dict.items():
        current_label = sentiment["label"]
        prompt = f"Given the following sentence from a financial document:\n\n\"{sentence}\"\n\nIn a financial context, please determine the sentiment. Respond with exactly one word—either positive, negative, or neutral—and nothing else."
        gpt_response = invoke_completions(prompt)
        corrected_label = gpt_response.strip().lower()
        if corrected_label not in ["positive", "negative", "neutral"]:
            corrected_label = current_label
        corrected[sentence] = {"original_label": current_label, "label": corrected_label, "gpt_response": gpt_response.strip()}
    return corrected

def process_sentiments_correction(sentiments_dict):
    corrected_results = {}
    for cik, filing_dict in sentiments_dict.items():
        corrected_results[cik] = {}
        for pair_key, category_sentiments in filing_dict.items():
            corrected_category = {}
            for category, sentiment_dict in category_sentiments.items():
                if sentiment_dict:
                    corrected_category[category] = correct_sentiments_with_gpt(sentiment_dict)
                else:
                    corrected_category[category] = {}
            corrected_results[cik][pair_key] = corrected_category
    return corrected_results

def aggregate_sentiment_scores(sentiment_dict):
    sentiment_mapping = {"positive": 1, "neutral": 0, "negative": -1, "unchanged": 0}
    aggregated = {}
    for category, sentence_sentiments in sentiment_dict.items():
        scores = []
        for sent, res in sentence_sentiments.items():
            label = res.get("label", "neutral")
            score = sentiment_mapping.get(label, 0)
            scores.append(score)
        avg = np.mean(scores) if scores else None
        aggregated[category] = {"avg_sentiment": avg, "scores": scores}
    return aggregated

def aggregate_for_all_filings(sentiments_dict):
    aggregated_results = {}
    for cik, filing_dict in sentiments_dict.items():
        aggregated_results[cik] = {}
        for pair_key, cat_scores in filing_dict.items():
            aggregated_results[cik][pair_key] = aggregate_sentiment_scores(cat_scores)
    return aggregated_results

def compute_weighted_overall_sentiment(aggregated_results, weights=None):
    if weights is None:
        weights = {"added": 1.0, "altered": 0.5, "removed": -1.0, "unchanged": 0.0}
    weighted_results = {}
    for cik, filing_dict in aggregated_results.items():
        weighted_results[cik] = {}
        for pair_key, cat_scores in aggregated_results[cik].items():
            total_weighted_score = 0.0
            total_abs_weight = 0.0
            for category, score_data in cat_scores.items():
                avg = score_data.get("avg_sentiment")
                if avg is not None:
                    w = weights.get(category, 0)
                    total_weighted_score += avg * w
                    total_abs_weight += abs(w)
            overall_sentiment = total_weighted_score / total_abs_weight if total_abs_weight > 0 else None
            weighted_results[cik][pair_key] = overall_sentiment
    return weighted_results

if __name__ == "__main__":
    os.chdir(pklpath)
    df = pd.read_pickle(pklpath + "f.pkl").reset_index(drop=True)
    categories_dict = process_filings_categories_chunked(df, model=mpnet, chunk_size=200, output_dir=pklpath, existing_pkl_file=None)
    with open("filings_sentence_categories.pkl", "wb") as f:
        pickle.dump(categories_dict, f)
    print("Phase 1 complete: Categorized sentences saved.")
    sentiments_dict = process_sentiments_without_gpt(categories_dict, finbert_pipeline=finbert)
    with open("filings_sentiment_finbert.pkl", "wb") as f:
        pickle.dump(sentiments_dict, f)
    print("Phase 2 complete: FinBERT sentiment analysis saved.")
    sentiments_corrected = process_sentiments_correction(sentiments_dict)
    with open("filings_sentiment_corrected.pkl", "wb") as f:
        pickle.dump(sentiments_corrected, f)
    print("Phase 3 complete: GPT-4 Turbo sentiment corrections saved.")
    aggregated_results = aggregate_for_all_filings(sentiments_corrected)
    print("Phase 4 complete: Aggregated sentiment results computed.")
    weighted_overall = compute_weighted_overall_sentiment(aggregated_results)
    print("Phase 5 complete: Weighted overall sentiment results computed.")
    print(json.dumps(weighted_overall, indent=2))
